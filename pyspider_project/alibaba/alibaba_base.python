#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Created on:2016/2/26 18:33
# Project:alibaba_base
# Author:yangmingsong

from pyspider.libs.base_handler import *
from ms_spider_fw.DBSerivce import DBService
import time
import re
import json

# config_text
# when start a spider,you should modify the next config text first

db_name = 'platform_data'  # database name for store data , string
table_name = 'alibaba_base'  # table name for store data , string
table_title = ',crawl_time'  # table title for store data , should be string separated by ','
url_start = 'http://www.1688.com'  # start url for crawl,string
# connect string , usually no need to modify
connect_dict = {'host': '10.118.187.12', 'user': 'admin', 'passwd': 'admin', 'charset': 'utf8'}

# now,the next is spider script
db_server = DBService(dbName=db_name, tableName=table_name, **connect_dict)
# if create table for store result in mysql , no need to be changed
# if not db_server.isTableExist():
#     db_server.createTable(tableTitle=table_title.split(','))

# re pattern compile
par_source_href = re.compile('"(http://s\.1688\.com/selloffer\S+?)"\.*?>(.+?)<')
par_href_parameters = re.compile('"hidden" value="(\S+?)" name="(\S+?)"')
pat_ajax_url = re.compile(r'data-mod-config.+?"url":"(.+?)"')


class Handler(BaseHandler):
    crawl_config = {
        'proxy': '10.10.10.10:80',
        'headers': {
            'User-Agent': 'User-Agent:Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1'
        }
    }

    @every(minutes=24 * 60)
    def on_start(self):
        self.crawl(url_start, callback=self.step_first)

    @config(age=2 * 24 * 60 * 60)
    def step_first(self, response):
        d = response.doc
        cate_0 = dict([(t.text(), t.attr('href')) for t in d('#nav-sub .fd-clr a').items()])
        for t in cate_0.items():
            self.crawl(t[1], callback=self.step_second, save={t[0], t[1]}, retries=100)

    def step_second(self, response):
        urls_t = dict(map(lambda x: (x[1], x[0]), re.findall(par_source_href, response.text)))
        s = response.save
        for t in urls_t.items():
            self.crawl(t[1], callback=self.step_third, save={'cate_0': s, 'cate_1': t[0]}, retries=100)

    def step_third(self, response):
        d = response.doc
        txt_t = response.text
        txt = re.sub('\s', '', txt_t).replace('\\','')
        # par_s=re.findall(par_href_parameters,response.text)
        Ajax_url = re.findall(pat_ajax_url, txt)[0]
        url_t = Ajax_url.replace('amp;', '')
        url = url_t if url_t[:3] == 'http' else 'http:' + url_t
        self.crawl(url, callback=self.step_third, retries=100)



    @config(priority=2)
    def my_result(self, response):
        d = response.doc
        crawl_time = time.strftime('%Y-%m-%d %X', time.localtime())
        return [
            crawl_time
        ]

    # over ride method for result store to mysql
    def on_result(self, result):
        if result:
            db_server.data2DB(data=result)
        else:
            print u'result-->return None'
